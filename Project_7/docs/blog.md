# âœ¨ Analyse de sentiments et dÃ©ploiement MLOps : retour dâ€™expÃ©rience complet

Dans le cadre du projet *Air Paradis*, nous avons mis en Å“uvre une chaÃ®ne complÃ¨te dâ€™analyse de sentiments Ã  partir de tweets, depuis la modÃ©lisation jusquâ€™au dÃ©ploiement MLOps. Ce billet de blog revient sur lâ€™ensemble des Ã©tapes rÃ©alisÃ©es, les choix mÃ©thodologiques, la comparaison des modÃ¨les et la mise en production dâ€™une API. ğŸ§ ğŸ”§

---

## ğŸ” Trois approches de modÃ©lisation supervisÃ©e

Nous avons testÃ© trois approches complÃ©mentaires pour prÃ©dire le sentiment dâ€™un tweet. Chacune sâ€™appuie sur un type de reprÃ©sentation des textes et un algorithme diffÃ©rent :

### 1. ğŸ§± Approche simple : Bag-of-Words (BoW) + modÃ¨les de machine learning

Cette approche utilise des vecteurs BoW ou BoW + TF-IDF, qui capturent la frÃ©quence des mots dans les tweets. On fait le choix dâ€™Ã©viter des techniques non supervisÃ©es comme **LDA**, car elles sont plus adaptÃ©es Ã  des tÃ¢ches de topic modeling et non de classification de sentiments (binaire qui plus est).

â¡ï¸ **ModÃ¨les testÃ©s :**  
- RÃ©gression Logistique  
- Naive Bayes  
- SVM  
- Random Forest  

Ces modÃ¨les ont Ã©tÃ© entraÃ®nÃ©s avec **GridSearchCV**, en optimisant la **prÃ©cision (`precision` et non `accuracy`)**, notre KPI principal.

ğŸ“¸ *[InsÃ©rer ici une capture dâ€™Ã©cran MLFlow avec les scores des modÃ¨les simples]*

---

### 2. ğŸ”¥ Approche avancÃ©e : Word Embeddings (Word2Vec & GloVe 300d) + LSTM

On reprÃ©sente les mots non plus comme des frÃ©quences mais comme des vecteurs denses, porteurs de signification.  
Nous avons utilisÃ© :
- **Word2Vec**, entraÃ®nÃ© sur un grand corpus (skip-gram),
- **GloVe 300d**, embeddings prÃ©calculÃ©s par Stanford.

Ces reprÃ©sentations sont **statistiquement proches** entre mots au sens sÃ©mantique.  
Elles **ne tiennent cependant pas compte du contexte exact dans la phrase** :  
> Â« banque Â» aura le mÃªme vecteur que ce soit pour une banque d'argent ou une banque de donnÃ©es.

La classification se fait via un **LSTM** (Long Short-Term Memory), une architecture de **deep learning supervisÃ©e** adaptÃ©e aux sÃ©quences de mots.

ğŸ“¸ *[InsÃ©rer ici une capture dâ€™Ã©cran du graphe de prÃ©cision sur validation pour GloVe+LSTM]*

---

### 3. ğŸ¤– BERT : Bidirectional Encoder Representations from Transformers

BERT est un modÃ¨le de langage avancÃ©, prÃ©entraÃ®nÃ© par Google, capable de **comprendre le contexte bidirectionnel** dâ€™un mot dans une phrase. Contrairement aux word embeddings classiques, BERT :
- **encode dynamiquement les mots** selon leur contexte dâ€™usage (ex. Â« banque Â» nâ€™aura pas le mÃªme vecteur selon la phrase),
- est entraÃ®nÃ© par **masked language modeling** et **next sentence prediction**, puis fine-tunÃ© pour la classification supervisÃ©e.

Cependant, il a ses **limites** :
> âš ï¸ BERT **ne comprend pas lâ€™ironie, le sarcasme ou les rÃ©fÃ©rences culturelles**. Il modÃ©lise des cooccurrences statistiques et non une comprÃ©hension intentionnelle ou implicite comme les LLM (ex : GPT).  
> Sur Twitter, oÃ¹ le second degrÃ© est omniprÃ©sent, cela reste un **vÃ©ritable dÃ©fi pour les modÃ¨les** de ce type.

ğŸ“¸ *[InsÃ©rer ici un graphique montrant la prÃ©cision de BERT sur le jeu de validation]*

---

## ğŸ“ Choix du KPI : pourquoi la prÃ©cision (precision) ?

Nous avons fait le choix de nous focaliser sur la **prÃ©cision** plutÃ´t que lâ€™**accuracy** :

> ğŸ¯ **Objectif mÃ©tier :** Ã©viter de **classer un tweet comme positif alors quâ€™il est nÃ©gatif**, car cela pourrait induire une mauvaise communication ou une mauvaise interprÃ©tation dans un contexte sensible.

### Pour chaque type de modÃ¨le :
- ModÃ¨les classiques â†’ optimisation via **GridSearchCV sur la prÃ©cision**  
- ModÃ¨les DL â†’ **EarlyStopping sur val_loss**, mais sÃ©lection du **meilleur modÃ¨le selon la prÃ©cision** sur lâ€™ensemble de validation.

---

## âš™ï¸ Mise en place de lâ€™environnement MLOps maison

Nous avons choisi une **infrastructure MLOps souveraine et autonome** pour orchestrer les expÃ©rimentations.

### ğŸ  Serveur personnel (NAS) + OpenMediaVault
Nous avons dÃ©ployÃ© MLFlow sur un NAS personnel tournant sous **OpenMediaVault** (basÃ© sur Debian).

### ğŸ“¦ Artefacts dans MinIO
Les modÃ¨les, mÃ©triques, visualisations et paramÃ¨tres sont stockÃ©s dans **MinIO**, un systÃ¨me compatible S3.  
â¡ï¸ MinIO est open-source, lÃ©ger, et permet une **gestion souveraine des artefacts**, contrairement Ã  AWS S3.

> Les notebooks (en particulier ceux entraÃ®nÃ©s sur Google Colab, pour bÃ©nÃ©ficier de GPU) envoient automatiquement leurs logs vers MLFlow et leurs artefacts vers MinIO, via des **variables dâ€™environnement** pour sÃ©curiser les identifiants.

ğŸ“¸ *[InsÃ©rer ici une capture du dashboard MLFlow]*

---

## ğŸ§ª IntÃ©gration continue, tests, versioning

Une fois les modÃ¨les validÃ©s, nous avons implÃ©mentÃ© toute la chaÃ®ne MLOps pour le dÃ©ploiement API :

### âœ… Tests automatisÃ©s
- **Tests unitaires** pour valider le prÃ©traitement des tweets, les prÃ©dictions et la gestion des erreurs.
- Ces tests sont intÃ©grÃ©s dans **GitHub Actions**, et **empÃªchent tout dÃ©ploiement Docker si les tests Ã©chouent**.

### ğŸ³ Conteneurisation & CI/CD
- Lâ€™API est packagÃ©e via **Docker**, puis poussÃ©e automatiquement sur **DockerHub** en cas de succÃ¨s.
- Cela permet un dÃ©ploiement cohÃ©rent et rapide sur nâ€™importe quelle plateforme.

---

## ğŸš€ DÃ©ploiement de lâ€™API FastAPI

### ğŸ› ï¸ Backend : FastAPI
- Fournit un endpoint `/predict`, avec chargement dynamique du modÃ¨le via MLFlow et rÃ©cupÃ©ration des embeddings (BoW, GloVe ou Word2Vec) selon le tag `embedding_type`.

### ğŸ’» Frontend local : Streamlit
- Sert dâ€™**interface utilisateur** pour tester les prÃ©dictions en local, simuler des requÃªtes API et visualiser les rÃ©sultats.  
- Câ€™est aussi un outil utile pour les dÃ©monstrations ou le debug.

ğŸ“¸ *[InsÃ©rer capture Streamlit en action]*

---

### ğŸŒ ProblÃ¨me matÃ©riel sur le NAS
Le NAS utilise un processeur **Intel N100**, thÃ©oriquement compatible AVX2.  
> â— Malheureusement, **certaines cartes mÃ¨res dÃ©sactivent cette instruction** au niveau BIOS, ce qui empÃªche lâ€™exÃ©cution de TensorFlow, nÃ©cessaire pour le LSTM et BERT.

ğŸ› ï¸ Solution : **dÃ©ploiement sur Google Cloud Run**, Ã  lâ€™adresse suivante :  
ğŸ”— [https://sentiment-api-service-7772256003.europe-west1.run.app/](https://sentiment-api-service-7772256003.europe-west1.run.app/)

ğŸ“¸ *[InsÃ©rer capture console Google Cloud / endpoint]*

---

## ğŸ“Š Suivi de performance en production : Azure Application Insights

### ğŸ¯ Objectif :
Mettre en place une **observabilitÃ©** pour surveiller lâ€™API une fois dÃ©ployÃ©e.

### ğŸ§© Suivi activÃ© :
- Traces des requÃªtes entrantes/sortantes
- Temps de rÃ©ponse
- Logs dâ€™erreurs

ğŸ“¸ *[InsÃ©rer capture Application Insight avec traces]*

### ğŸ” Et aprÃ¨s ? StratÃ©gie dâ€™amÃ©lioration continue

Voici une **dÃ©marche MLOps** possible pour affiner le modÃ¨le :

1. **Suivi des erreurs en production** (ex. sentiment mal classÃ© identifiÃ© par des utilisateurs)
2. **Stockage de ces cas edge** dans une base dÃ©diÃ©e
3. **Retrain du modÃ¨le pÃ©riodique** avec ces cas en plus
4. Comparaison des performances sur une **valset figÃ©e**
5. **DÃ©ploiement automatique** dâ€™un nouveau modÃ¨le si KPIs dÃ©passÃ©s

---

## âœ¨ Conclusion

Ce projet a permis dâ€™explorer en profondeur les enjeux de classification de sentiments et les **limites des approches standards face Ã  la richesse du langage naturel**, en particulier sur Twitter.  
Il a aussi dÃ©montrÃ© lâ€™intÃ©rÃªt dâ€™un **pipeline MLOps souverain**, reproductible et automatisÃ©.  

> âœ… En alliant des modÃ¨les puissants, des outils robustes comme MLFlow et FastAPI, et un dÃ©ploiement maÃ®trisÃ©, on pose les bases dâ€™un produit de NLP industrialisable.

ğŸ“¸ *[InsÃ©rer ici un schÃ©ma global du pipeline ou architecture finale]*

---

ğŸ’¬ Merci pour votre lecture !  
Pour tester lâ€™API : [https://sentiment-api-service-7772256003.europe-west1.run.app/](https://sentiment-api-service-7772256003.europe-west1.run.app/)

