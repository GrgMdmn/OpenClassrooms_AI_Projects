# âœ¨ Analyse de sentiments et dÃ©ploiement MLOps : retour dâ€™expÃ©rience complet

Dans le cadre du projet *Air Paradis*, nous avons mis en Å“uvre une chaÃ®ne complÃ¨te dâ€™analyse de sentiments Ã  partir de tweets, depuis la modÃ©lisation jusquâ€™au dÃ©ploiement MLOps. Ce billet de blog revient sur les Ã©tapes clÃ©s du projet : choix mÃ©thodologiques, comparaison des modÃ¨les, mise en production dâ€™une API robuste, et dÃ©ploiement sur deux environnements (local + cloud). ğŸ§ ğŸ”§

---

## ğŸ” Trois approches de modÃ©lisation supervisÃ©e

Nous avons testÃ© trois approches complÃ©mentaires pour prÃ©dire le sentiment dâ€™un tweet. Chacune repose sur une reprÃ©sentation textuelle diffÃ©rente et un algorithme dâ€™apprentissage adaptÃ© :

### 1. ğŸ§± Approche simple : Bag-of-Words (BoW) + modÃ¨les de machine learning

On vectorise les tweets via BoW ou BoW + TF-IDF, qui comptent la frÃ©quence des mots.  
Contrairement Ã  d'autres projets NLP, nous avons Ã©vitÃ© LDA (non supervisÃ©, orientÃ© topic modeling).

â¡ï¸ **ModÃ¨les testÃ©s** :
- RÃ©gression Logistique  
- Naive Bayes  
- SVM  
- Random Forest  

Ces modÃ¨les sont entraÃ®nÃ©s avec **GridSearchCV**, en optimisant la **prÃ©cision (`precision`)**, notre KPI mÃ©tier. DiffÃ©rents KPI ont Ã©tÃ© calculÃ©s sur l'ensemble de test.

![Scores modÃ¨les simples](./Basic-Models.png)

---

### 2. ğŸ”¥ Approche avancÃ©e : Word Embeddings + LSTM

Nous utilisons des vecteurs denses Word2Vec (entraÃ®nÃ©s) et GloVe 300d (prÃ©entraÃ®nÃ©s par Stanford), qui capturent les similaritÃ©s sÃ©mantiques entre mots. Ces reprÃ©sentations ne prennent toutefois **pas en compte le contexte** exact (ex : "annulÃ©" est probablement un mot trÃ¨s nÃ©gatif pour un vol mais positif pour un procÃ¨s contenant des charges contre vous).

La classification est rÃ©alisÃ©e par un **LSTM**, capable d'exploiter la sÃ©quentialitÃ© des mots.  
Le **modÃ¨le dÃ©ployÃ© est automatiquement sÃ©lectionnÃ©** entre Word2Vec + LSTM et GloVe + LSTM, selon la **prÃ©cision maximale enregistrÃ©e dans MLFlow**.

ModÃ¨le `Word2Vec + LSTM` (`test_precision : 0.797 , val_precision : 0.795`):
![Scores entrainement modÃ¨le avancÃ© LSTM + Word2Vec](./Word2Vec_LSTM_epoch9.png)
ModÃ¨le `GloVe 300d + LSTM` (`test_precision : 0.792 , val_precision : 0.798`):
![Scores entrainement modÃ¨les avancÃ©s LSTM + GloVe 300d](./GloVe-300d_LSTM_epoch12.png)

Bien que GloVe 300d + LSTM prÃ©sente un score de prÃ©cision lÃ©gÃ¨rement supÃ©rieur Ã  Word2Vec + LSTM sur l'ensemble de validation, il reste infÃ©rieur pour l'ensemble de test (relativement nÃ©gligeable dans l'absolu).

---

### 3. ğŸ¤– Approche transformer : DistilBERT, embeddings contextuels allÃ©gÃ©s

Nous avons utilisÃ© **DistilBERT**, une version plus lÃ©gÃ¨re et plus rapide de BERT (crÃ©Ã©e par Hugging Face).  
Il conserve **95â€¯% de la performance de BERT** tout en Ã©tant **40â€¯% plus petit** et **60â€¯% plus rapide**, ce qui le rend particuliÃ¨rement adaptÃ© Ã  un **entraÃ®nement sur GPU limitÃ©** (Google Colab) et Ã  une **infÃ©rence plus rapide**.

Comme BERT, DistilBERT produit des **embeddings contextuels**, câ€™est-Ã -dire que le sens des mots varie selon leur contexte. Par exemple, *"banque"* dans *"je vais Ã  la banque"* et *"banque de donnÃ©es"* nâ€™aura pas le mÃªme vecteur.

Il a Ã©tÃ© **fine-tunÃ©** ici pour une classification binaire (positif/nÃ©gatif), avec un tokenizer et un encodage spÃ©cifiques Ã  son architecture.

> **ModÃ¨le `DistilBERT`** (`test_precision : 0.861 , val_precision : 0.858`)

![Courbes d'entraÃ®nement DistilBERT](./bert_training_curves_epoch1.png)  
![Matrice de Confusion](./bert_confusion_matrix.png)  
![ROC Curve](./bert_roc_curve.png)

âš ï¸ **Limites** : comme tous les modÃ¨les prÃ©entraÃ®nÃ©s, DistilBERT **nâ€™est pas sensible Ã  lâ€™ironie, aux jeux de mots, ni aux rÃ©fÃ©rences culturelles implicites**.  
Or, Twitter est un terrain de jeu idÃ©al pour ce genre de contenus dÃ©tournÃ©s. Ces limites sâ€™appliquent aussi Ã  Word2Vec, GloVe et aux modÃ¨les classiques.

---

## ğŸ¯ Pourquoi optimiser la prÃ©cision (et non l'accuracy) ?

**Accuracy** = proportion de bonnes prÃ©dictions globales.  
**Precision** = proportion de tweets *prÃ©vus positifs* qui sont *vraiment positifs*.

> Exemple : sur 100 tweets, si le modÃ¨le prÃ©dit 30 positifs, mais 15 sont des erreurs, la prÃ©cision est de 50â€¯%.

Nous avons prÃ©fÃ©rÃ© **minimiser les faux positifs**, car dans un contexte sensible, **annoncer un tweet positif alors quâ€™il est nÃ©gatif peut Ãªtre problÃ©matique**.

- ModÃ¨les classiques : **GridSearchCV optimisÃ© sur la prÃ©cision**  
- ModÃ¨les DL : **EarlyStopping sur val_loss**, mais **sÃ©lection finale sur la prÃ©cision**

---

## âš™ï¸ Environnement MLOps souverain

### ğŸ–¥ï¸ Serveur personnel NAS + OpenMediaVault

MLFlow est installÃ© sur un **NAS personnel (Intel N100)**, sous **OpenMediaVault (Debian)**.  
Nous utilisons **MinIO** pour stocker artefacts et modÃ¨les, garantissant une gestion locale, souveraine et compatible S3.

> Les notebooks, exÃ©cutÃ©s sur Google Colab pour bÃ©nÃ©ficier de GPU, loggent dans MLFlow et envoient les modÃ¨les vers MinIO via des **variables dâ€™environnement**.

Dashboard ExpÃ©riences MLFlow :
![Dashboard MLFLow](./mlflow_experiments_dashboard.png)

ModÃ¨les en production (registre de modÃ¨les):
![Models Registry](mlflow_models_registry_2-1.png)

Meilleur modÃ¨le non BERT retenu :
![ModÃ¨le retenu](mlflow_models_registry_best_LSTM_model-1.png)
![Meilleur modÃ¨le non BERT](mlflolw_experiments_dashboard_best_LSTM_model-1.png)
---

## ğŸ§ª CI, tests et versioning

### âœ… Tests unitaires

- VÃ©rifient le bon fonctionnement du prÃ©traitement, des prÃ©dictions et de la gestion dâ€™erreurs.
- IntÃ©grÃ©s Ã  **GitHub Actions**, **ils bloquent le dÃ©ploiement si les tests Ã©chouent.**

### ğŸ³ Conteneurisation & CI/CD

Lâ€™API FastAPI est packagÃ©e via Docker, puis poussÃ©e sur DockerHub.  
Un pipeline CI/CD complet garantit **un dÃ©ploiement reproductible**.

![github workflow](github_ci-cd-1.png)
---

## ğŸš€ DÃ©ploiement de lâ€™API FastAPI (double)

### ğŸ”§ Backend : FastAPI

- Expose un endpoint `/predict`  
- Charge dynamiquement le **meilleur modÃ¨le Word Embedding + LSTM** via le *Model Registry* MLFlow  
- Applique les bons embeddings selon le tag `embedding_type`

### ğŸ’» Interface Streamlit

Interface locale pour tester les requÃªtes, debug, dÃ©monstrations.

![Streamlit Local](./streamlit_interface_local.png)

---

### ğŸŒ Double dÃ©ploiement (NAS + Google Cloud)

#### ğŸ”§ ProblÃ¨me AVX2

Initialement, lâ€™API ne fonctionnait pas sur le NAS (Intel N100), faute dâ€™instruction **AVX2**, requise par TensorFlow.

> ğŸ› ï¸ **Solution : activer AVX2 dans le BIOS** (dÃ©sactivÃ©e par dÃ©faut sur certaines cartes mÃ¨res, souvent optimisÃ©es pour NAS).

Depuis, lâ€™API tourne sur :
- ğŸ  NAS local : [sentiment-api.greg-madman-nas.duckdns.org](https://sentiment-api.greg-madman-nas.duckdns.org)\
Interface Docker Compose de NAS:
![docker compose openmediavault](openmediavault_docker-compose_settings-1.png)

- â˜ï¸ Google Cloud Run : [sentiment-api-service-7772256003.europe-west1.run.app](https://sentiment-api-service-7772256003.europe-west1.run.app)\
Interface DÃ©ploiement Google Cloud:
![dÃ©ploiement google cloud](google_cloud_interface-1.png)

---

## ğŸ“Š Monitoring maison

Une logique dâ€™alerte lÃ©gÃ¨re embarquÃ©e dans lâ€™API :

![1Ã¨re erreur](./api_first_error.png)

![2Ã¨me erreur](./api_second_error.png)

![3Ã¨me erreur](./api_third_error.png)


Si **3 erreurs de prÃ©diction** consÃ©cutives sont dÃ©tectÃ©es sur une **fenÃªtre de 5 min**, un rapport est gÃ©nÃ©rÃ© automatiquement.

![Rapport d'erreur envoyÃ© par mailÂ²](./api_error_report.png)



---

## ğŸ§  SchÃ©ma global du pipeline MLOps

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  DonnÃ©es   â”‚
     â”‚ (tweets)   â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    PrÃ©traitement (regex, tokenisation, lemmatisation, stopwords)
           â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  EntraÃ®nement ML   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    Log via MLFlow + artefacts MinIO
           â”‚
 SÃ©lection du meilleur modÃ¨le "avancÃ©" (hors BERT) selon prÃ©cision
           â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚    API FastAPI     â”‚
 â”‚    + Streamlit     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
 DÃ©ploiement sur NAS + Google Cloud Run
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Monitoring lÃ©ger maisonâ”‚
â”‚ (alertes sur mauvaises â”‚
â”‚       prÃ©dictions)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Conclusion

Ce projet dÃ©montre comment construire une chaÃ®ne MLOps complÃ¨te, autonome et reproductible pour du NLP appliquÃ© Ã  Twitter.  
Il souligne les **forces et limites des modÃ¨les** (y compris BERT), la nÃ©cessitÃ© dâ€™une mÃ©trique adaptÃ©e au contexte (prÃ©cision vs accuracy), et lâ€™importance dâ€™un **dÃ©ploiement maÃ®trisÃ©**, mÃªme sur une infrastructure personnelle.

---

ğŸ’¬ Merci pour votre lecture !