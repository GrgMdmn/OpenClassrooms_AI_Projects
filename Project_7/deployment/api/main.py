import os
import sys
import pickle
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import mlflow
from mlflow.tracking import MlflowClient
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# A DECOMMENTER POUR DEVELOPPER. AUTREMENT, LAISSER AINSI 
# Acc√©der √† .env
PARENT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
sys.path.append(PARENT_DIR)
# üîê Charger les variables d'environnement (.env √† deux niveaux au-dessus)
load_dotenv()


# üîÅ Import de la fonction de pr√©traitement (LOCAL)
from preprocessing import preprocess_tweet

# üìß Import du service email
from email_service import send_error_report_email


# üîó Configurer MLflow
mlflow_tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
if mlflow_tracking_uri:
    mlflow.set_tracking_uri(mlflow_tracking_uri)
else:
    print("‚ö†Ô∏è MLFLOW_TRACKING_URI non d√©fini dans .env")

# üìå Nom du mod√®le et stage
MODEL_NAME = "SentimentAnalysisLSTM"
STAGE = "Production"

# üßæ Structures des requ√™tes POST
class TweetRequest(BaseModel):
    tweet: str

class ReportRequest(BaseModel):
    tweet: str
    prediction: str
    probability: float

# üß† Stockage des signalements d'erreurs
error_reports = {}

# üîç Fonction pour charger mod√®le + tokenizer
def load_model_and_artifacts(model_name=MODEL_NAME, stage=STAGE):
    client = MlflowClient()
    versions = client.get_latest_versions(model_name, [stage])
    
    if not versions:
        raise RuntimeError(f"Aucune version du mod√®le '{model_name}' en stage '{stage}'")
    
    version_info = versions[0]
    run_id = version_info.run_id
    
    if not run_id:
        raise RuntimeError("Run ID introuvable.")
    
    # Extraire le tag d'embedding
    model_version_info = client.get_model_version(model_name, version_info.version)
    # glove est ici la valeur par d√©faut si embedding_type n'est pas trouv√©.
    embedding_type = model_version_info.tags.get("embedding_type", "glove").lower()
    
    # D√©terminer les chemins des artefacts
    local_dir = "./downloaded_artifacts"
    os.makedirs(local_dir, exist_ok=True)
    print("Contenu de downloaded_artifacts :", os.listdir(local_dir))
    base_path = "local_artifacts"
    tokenizer_file = "tokenizer.pickle"
    
    if "glove" in embedding_type:
        keras_file = "final_model_LSTM_GloVe-300d-Fige.keras"
    elif "word2vec" in embedding_type:
        keras_file = "final_model_LSTM_Word2Vec-Fige.keras"
    else:
        raise ValueError(f"Embedding type inconnu : {embedding_type}")
        
    print(f"Mod√®le {embedding_type} trouv√©. T√©l√©chargement du mod√®le li√© au run_id suivant : {run_id}")
    
    def dl(path):
        return client.download_artifacts(run_id=run_id, path=f"{base_path}/{path}", dst_path=local_dir)
    
    # üîΩ T√©l√©charger les artefacts
    model_path = dl(keras_file)
    tokenizer_path = dl(tokenizer_file)
    
    # üì¶ Chargement
    model = load_model(model_path)
    with open(tokenizer_path, "rb") as f:
        tokenizer = pickle.load(f)
    
    return model, tokenizer, embedding_type

# üöÄ Lancer l'API
app = FastAPI(title="API Pr√©diction Sentiment - Air Paradis")
# app = FastAPI(title="API Pr√©diction Sentiment - Air Paradis",
#               root_path="/api")

try:
    model, tokenizer, embedding_type = load_model_and_artifacts()
except Exception as e:
    print(f"\n‚ùå Erreur lors du chargement des artefacts : {e}")
    model, tokenizer = None, None

@app.get("/")
def root():
    return {"message": "API Sentiment - Air Paradis - en ligne"}

@app.post("/predict")
def predict_sentiment(request: TweetRequest):
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="Mod√®le ou artefacts indisponibles.")
    
    try:
        processed_tweet = preprocess_tweet(request.tweet)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©traitement : {e}")
    
    if not processed_tweet.strip():
        raise HTTPException(status_code=400, detail="Tweet vide ou invalide apr√®s pr√©traitement.")
    
    try:
        sequence = tokenizer.texts_to_sequences([processed_tweet])
        sequence_padded = pad_sequences(sequence, maxlen=model.input_shape[1])
        prediction_prob = model.predict(sequence_padded)[0][0]
        if prediction_prob >=0.5:
            sentiment = "positif"
        else:
            sentiment = "negatif"
            # prediction_prob = 1-prediction_prob
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur d'inf√©rence : {e}")
    
    return {
        "sentiment": sentiment,
        "probability": float(prediction_prob)
    }

@app.post("/report_error")
def report_error(request: ReportRequest):
    tweet = request.tweet.strip()
    prediction = request.prediction.strip().lower()
    probability = request.probability

    if not tweet or not prediction:
        raise HTTPException(status_code=400, detail="Tweet ou pr√©diction manquants.")
    if probability < 0 or probability > 1:
        raise HTTPException(status_code=400, detail="Probabilit√© invalide. Elle doit √™tre entre 0 et 1.")

    # Ajouter ou mettre √† jour le signalement du tweet avec la probabilit√©
    error_reports[tweet] = prediction + f' (p = {probability:.2f} ) '

    report_sent = False

    # Condition : √Ä chaque multiple de 3 signalements, un mail est envoy√©
    try:
        if len(error_reports) % 3 == 0: # La condition reste
            report_sent = True
            print(f"üìß Tentative d'envoi d'email pour {len(error_reports)} signalements...")
            email_success = send_error_report_email(error_reports.copy())
            if email_success:
                print(f"‚úÖ Email envoy√© avec succ√®s pour {len(error_reports)} signalements")
            else:
                print(f"‚ùå √âchec de l'envoi de l'email pour {len(error_reports)} signalements")
    except Exception as e:
        print(f"‚ùå Erreur lors de l'envoi de l'email : {e}")
        # On continue m√™me si l'email √©choue pour ne pas bloquer l'API

    return {"report_sent": report_sent}